[
  {
    "objectID": "reference-en.html",
    "href": "reference-en.html",
    "title": "Function reference",
    "section": "",
    "text": "Here, we have a list of all documented functions in the spark_map package.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nall_of()\n\n\n\n\n\n\nare_of_type()\n\n\n\n\n\n\nat_position()\n\n\n\n\n\n\nends_with()\n\n\n\n\n\n\nmatches()\n\n\n\n\n\n\nspark_across()\n\n\n\n\n\n\nspark_map()\n\n\n\n\n\n\nstarts_with()\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "how-to-test.html",
    "href": "how-to-test.html",
    "title": "How to test the package",
    "section": "",
    "text": "The spark_map package uses the pytest framework to perform the unit tests of the package. Because of that, you need to have the pytest package installed on your machine. To install it, you can use pip:\npip install pytest\nAfter you make sure that pytest is installed, you can run the tests. To run the unit tests of the package, you just need to run the pytest command on the terminal:\npytest"
  },
  {
    "objectID": "how-to-test.html#remember-to-include-the-current-directory-in-the-search-path",
    "href": "how-to-test.html#remember-to-include-the-current-directory-in-the-search-path",
    "title": "How to test the package",
    "section": "Remember to include the current directory in the search path",
    "text": "Remember to include the current directory in the search path\nTo test the package, you have to make sure that the root directory of the project of the package is included in the search path of the python interpreter (i.e. the PYTHONPATH variable). To do make life easier, the following rule is included in the pyproject.toml file:\n[tool.pytest.ini_options]\npythonpath = [\n  \".\"\n]\nWith the above rule, the root directory of the project should be included automatically when you run the tests with the pytest command. You can read more about this rule in the pytest documentation1."
  },
  {
    "objectID": "reference-ptbr.html",
    "href": "reference-ptbr.html",
    "title": "Referência de funções",
    "section": "",
    "text": "Nesta página, temos uma lista de todas as funções documentadas no pacote spark_map.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introducing spark_map",
    "section": "",
    "text": "spark_map is a python package that offers some tools to easily apply a function over multiple columns of Apache Spark DataFrames, using pyspark. You could say that spark_map offers an implementation for the map() python function for Spark DataFrames. There are two main functions in the package that performs the heavy work, which are spark_map() and spark_across().\nBoth of these functions perform the same work, which is to apply a function over multiple columns of a Spark DataFrame. But they differ in the method they use to apply this function. spark_map() uses the agg() method of Spark DataFrame’s to apply the function, and spark_across() uses the withColumn() method to do so.\nThis means that you will mainly use spark_map() when you want to calculate aggregates of each column. Is worthy pointing out that spark_map() works perfectly with grouped DataFrames as well (i.e. GroupedData). In the other hand, you will use spark_across() when you want to just transform the values of multiple colums at once by applying the same function over them."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Introducing spark_map",
    "section": "Installation",
    "text": "Installation\nTo get the latest version of spark_map at PyPI, use:\npip install spark-map"
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Introducing spark_map",
    "section": "Documentation",
    "text": "Documentation\nThe full documentation for spark_map package is available at this website. To access it, just use the Function Reference and Articles menus located at the top navigation bar of this page."
  },
  {
    "objectID": "index.html#a-simple-example-of-use",
    "href": "index.html#a-simple-example-of-use",
    "title": "Introducing spark_map",
    "section": "A simple example of use",
    "text": "A simple example of use\nAs an example, consider the students DataFrame below:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nd = [\n  (12114, 'Anne', 21, 1.56, 8, 9, 10, 9, 'Economics', 'SC'),\n  (13007, 'Adrian', 23, 1.82, 6, 6, 8, 7, 'Economics', 'SC'),\n  (10045, 'George', 29, 1.77, 10, 9, 10, 7, 'Law', 'SC'),\n  (12459, 'Adeline', 26, 1.61, 8, 6, 7, 7, 'Law', 'SC'),\n  (10190, 'Mayla', 22, 1.67, 7, 7, 7, 9, 'Design', 'AR'),\n  (11552, 'Daniel', 24, 1.75, 9, 9, 10, 9, 'Design', 'AR')\n]\n\ncolumns = [\n  'StudentID', 'Name', 'Age', 'Height', 'Score1',\n  'Score2', 'Score3', 'Score4', 'Course', 'Department'\n]\n\nstudents = spark.createDataFrame(d, columns)\nstudents.show(truncate = False)\n\nSuppose you want to calculate the average of the third, fourth and fifth columns of this DataFrame students. The spark_map() function allows you to perform this calculation in an extremely simple and clear way, as shown below:\n\nfrom pyspark.sql.functions import mean\nfrom spark_map import spark_map, at_position\n\nspark_map(students, at_position(3, 4, 5), mean).show(truncate = False)\n\nIf you want your calculation to be applied by group, just provide the grouped table to spark_map(). For example, suppose you wanted to calculate the same averages as in the example above, but within each department:\n\nby_department = students.groupBy('Department')\nspark_map(by_department, at_position(3, 4, 5), mean).show()"
  },
  {
    "objectID": "how-to-build.html",
    "href": "how-to-build.html",
    "title": "How to build the package",
    "section": "",
    "text": "To build the spark_map package, you need to first, clone the repository to your machine (with git clone), and change to the root directory of the project (with cd):\ngit clone https://github.com/pedropark99/spark_map.git\ncd spark_map\nAfter that, you use the standard build command for python packages. As a result, a dist folder will be created in the current directory, and the compiled files for the package will be inside of it.\n# If you are in windows\npy -m build\n# If you are in MacOs/Linux\npython3 -m build"
  },
  {
    "objectID": "articles-en.html",
    "href": "articles-en.html",
    "title": "Articles",
    "section": "",
    "text": "Here, we have a list of all articles (i.e. long documentations) about the spark_map package.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nBuilding the mapping\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "english/matches.html",
    "href": "english/matches.html",
    "title": "matches()",
    "section": "",
    "text": "Map all columns of your Spark DataFrame that fit into a regular expression. This function is one of several existing mapping functions (read the article “Building the mapping”)."
  },
  {
    "objectID": "english/matches.html#arguments",
    "href": "english/matches.html#arguments",
    "title": "matches()",
    "section": "Arguments",
    "text": "Arguments\n\nregex: a string (preferably a raw string) containing the regular expression to be used;"
  },
  {
    "objectID": "english/matches.html#details-and-examples",
    "href": "english/matches.html#details-and-examples",
    "title": "matches()",
    "section": "Details and examples",
    "text": "Details and examples\nTherefore, matches() is used to define which columns spark_map() will apply the given function to. To use this function, you supply a raw string containing the regular expression you want to use. It is extremely important that you provide your expression within a raw string rather than a traditional string, especially if your expression includes special characters like TABs or new lines ('\\t' or '\\n'). In Python, raw strings are constructed by placing an 'r' before the quotes in our string. Therefore, the expression r'raw string' represents a raw string, while 'string' represents a traditional string.\nIt is worth noting that the regular expression provided will be passed to the re.match() method, and will be applied to the name of each column of your Spark DataFrame. With that in mind, if your regular expression can’t find any column, it’s interesting that you investigate your error through the re.match() method. For example, suppose you have the DataFrame pop below. Suppose also that you want to map all columns that contain the string 'male' somewhere. Note that no columns were found by spark_map().\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max\nfrom spark_map.functions import spark_map\nfrom spark_map.mapping import matches\nspark = SparkSession.builder.getOrCreate()\nimport re\n\ndata = [\n  ('Brazil', 74077777, 86581634, 96536269, 74925448, 88208705, 99177368),\n  ('Colombia', 16315306, 19427307, 22159658, 16787263, 20202658, 23063041),\n  ('Russia', 69265950, 68593139, 66249411, 78703457, 78003730, 76600057)\n]\n\npop = spark.createDataFrame(\n  data,\n  ['country', 'pop_male_1990', 'pop_male_2000', 'pop_male_2010',\n   'pop_female_1990', 'pop_female_2000', 'pop_female_2010']\n)\n\n\nspark_map(pop, matches(r'male'), max).show()\n\nKeyError: '`spark_map()` did not found any column that matches your mapping!'\n\n\nTo investigate what is going wrong in this case, it is useful to separate the name of a column that should have been found and apply re.match() in isolation to that column. Note below that the result of the expression re.match(r'male', name) is None. This means that the regular expression 'male' does not generate a match with the text pop_male_1990.\n\nname = 'pop_male_1990'\nprint(re.match(r'male', name))\n\nNone\n\n\nBy testing various combinations and delving deeper into the problem, you may eventually find that the expression 'male' is wrong as it represents an exact match with the text 'male'. That is, with this expression, re.match() is able to find only the text 'male' and nothing else. We can fix this problem by allowing an arbitrary number of characters to be found around the text 'male'. For this, we circumvent 'male' with the mini-expression '(.+)', as shown below:\n\nname = 'pop_male_1990'\nprint(re.match(r'(.+)male(.+)', name))\n\n<re.Match object; span=(0, 13), match='pop_male_1990'>\n\n\nNow that we’ve tested this new regular expression in re.match() we can return to the matches() function. Notice below that this time all the expected columns are found.\n\nspark_map(pop, matches(r'(.+)male(.+)'), max).show()\n\nSelected columns by `spark_map()`: pop_male_1990, pop_male_2000, pop_male_2010, pop_female_1990, pop_female_2000, pop_female_2010\n\n\n\n[Stage 0:>                                                        (0 + 12) / 12]\n\n\n+-------------+-------------+-------------+---------------+---------------+---------------+\n|pop_male_1990|pop_male_2000|pop_male_2010|pop_female_1990|pop_female_2000|pop_female_2010|\n+-------------+-------------+-------------+---------------+---------------+---------------+\n|     74077777|     86581634|     96536269|       78703457|       88208705|       99177368|\n+-------------+-------------+-------------+---------------+---------------+---------------+"
  },
  {
    "objectID": "english/all_of.html",
    "href": "english/all_of.html",
    "title": "all_of()",
    "section": "",
    "text": "Map all columns of your Spark DataFrame whose name is included in a string list. This function is one of several existing mapping functions (read the article “Building the mapping”)."
  },
  {
    "objectID": "english/all_of.html#arguments",
    "href": "english/all_of.html#arguments",
    "title": "all_of()",
    "section": "Arguments",
    "text": "Arguments\n\nlist_cols: a list of strings containing the names of the columns you want to map;"
  },
  {
    "objectID": "english/all_of.html#details-and-examples",
    "href": "english/all_of.html#details-and-examples",
    "title": "all_of()",
    "section": "Details and examples",
    "text": "Details and examples\nTherefore, all_of() is used to define which columns spark_map() will apply the given function to. You can use this function, when you want to allow a set of columns to be mapped, but for some reason you don’t know in advance if all these columns (or a part of them) will be available in your Spark DataFrame.\nYou must give all_of() a list of strings. Each string represents the name of a column that can be mapped. As an example, the expression all_of(['sales_france', 'sales_brazil', 'sales_colombia']) allows columns named \"sales_france\", \"sales_brazil\" and \"sales_colombia\" to be mapped by spark_map(). However, spark_map() doesn’t necessarily need to find all these columns at once. That is, all_of() makes these columns “optional”, so spark_map() can find all three columns, or only two, or even just one of these columns. See the example below:\n\nfrom pyspark.sql.functions import mean\nfrom spark_map.functions import spark_map\nfrom spark_map.mapping import all_of\n\ndata = [\n  (2022, 1, 12300, 41000, 36981),\n  (2022, 2, 19120, 21300, 31000),\n  (2022, 3, 18380, 11500, 31281)\n]\n\nsales = spark.createDataFrame(data, ['year', 'month', 'sales_france', 'sales_brazil', 'sales_russia'])\n\nspark_map(\n    sales,\n    all_of(['sales_france', 'sales_brazil', 'sales_colombia']),\n    mean\n  )\\\n  .show()\n\nSelected columns by `spark_map()`: sales_france, sales_brazil\n\n\n\n[Stage 0:>                                                        (0 + 12) / 12]\n\n\n+------------+------------+\n|sales_france|sales_brazil|\n+------------+------------+\n|     16600.0|     24600.0|\n+------------+------------+\n\n\n\n                                                                                \n\n\nHowever, it is worth noting that spark_map() must find at least one of the columns defined in all_of(). If it doesn’t, spark_map() will raise a KeyError warning that no column could be found with the mapping you defined.\n\nspark_map(sales, all_of(['sales_italy']), mean).show()\n\nKeyError: '`spark_map()` did not found any column that matches your mapping!'"
  },
  {
    "objectID": "english/spark_map.html",
    "href": "english/spark_map.html",
    "title": "spark_map()",
    "section": "",
    "text": "With spark_map() you are easily apply a aggregate function over multiple columns of a Spark DataFrame (i.e. DataFrame), or a grouped Spark DataFrame (i.e. GroupedData). In short, spark_map() use the agg() DataFrame method to apply your function over the mapped columns. This means that spark_map() takes a Spark DataFrame as input and returns a new Spark DataFrame (aggregated by the function you provided) as output."
  },
  {
    "objectID": "english/spark_map.html#arguments",
    "href": "english/spark_map.html#arguments",
    "title": "spark_map()",
    "section": "Arguments",
    "text": "Arguments\n\ntable: a Spark DataFrame or a grouped DataFrame (i.e. pyspark.sql.dataframe.DataFrame or pyspark.sql.group.GroupedData);\nmapping: the mapping that defines the columns where you want to apply function (read the article “Building the mapping”);\nfunction: the function you want to apply to each column defined in mapping;"
  },
  {
    "objectID": "english/spark_map.html#details-and-examples",
    "href": "english/spark_map.html#details-and-examples",
    "title": "spark_map()",
    "section": "Details and examples",
    "text": "Details and examples\nAs an example, consider the students DataFrame below:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nd = [\n  (12114, 'Anne', 21, 1.56, 8, 9, 10, 9, 'Economics', 'SC'),\n  (13007, 'Adrian', 23, 1.82, 6, 6, 8, 7, 'Economics', 'SC'),\n  (10045, 'George', 29, 1.77, 10, 9, 10, 7, 'Law', 'SC'),\n  (12459, 'Adeline', 26, 1.61, 8, 6, 7, 7, 'Law', 'SC'),\n  (10190, 'Mayla', 22, 1.67, 7, 7, 7, 9, 'Design', 'AR'),\n  (11552, 'Daniel', 24, 1.75, 9, 9, 10, 9, 'Design', 'AR')\n]\n\ncolumns = [\n  'StudentID', 'Name', 'Age', 'Height', 'Score1',\n  'Score2', 'Score3', 'Score4', 'Course', 'Department'\n]\n\nstudents = spark.createDataFrame(d, columns)\nstudents.show(truncate = False)\n\n[Stage 0:>                                                          (0 + 1) / 1]\n\n\n                                                                                \n\n\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|StudentID|Name   |Age|Height|Score1|Score2|Score3|Score4|Course   |Department|\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|12114    |Anne   |21 |1.56  |8     |9     |10    |9     |Economics|SC        |\n|13007    |Adrian |23 |1.82  |6     |6     |8     |7     |Economics|SC        |\n|10045    |George |29 |1.77  |10    |9     |10    |7     |Law      |SC        |\n|12459    |Adeline|26 |1.61  |8     |6     |7     |7     |Law      |SC        |\n|10190    |Mayla  |22 |1.67  |7     |7     |7     |9     |Design   |AR        |\n|11552    |Daniel |24 |1.75  |9     |9     |10    |9     |Design   |AR        |\n+---------+-------+---+------+------+------+------+------+---------+----------+\n\n\n\nSuppose you want to calculate the average of the third, fourth and fifth columns of this DataFrame students. The spark_map() function allows you to perform this calculation in an extremely simple and clear way, as shown below:\n\nimport pyspark.sql.functions as F\nfrom spark_map.functions import spark_map\nfrom spark_map.mapping import at_position\nspark_map(students, at_position(3, 4, 5), F.mean).show(truncate = False)\n\nSelected columns by `spark_map()`: Age, Height, Score1\n\n\n\n+------------------+------------------+------+\n|Age               |Height            |Score1|\n+------------------+------------------+------+\n|24.166666666666668|1.6966666666666665|8.0   |\n+------------------+------------------+------+\n\n\n\nIf you want your calculation to be applied by group, just provide the grouped DataFrame to spark_map(). For example, suppose you wanted to calculate the same averages as in the example above, but within each department:\n\nby_department = students.groupBy('Department')\nspark_map(by_department, at_position(3, 4, 5), F.mean).show()\n\nSelected columns by `spark_map()`: Age, Height, Score1\n\n\n\n+----------+-----+------------------+------+\n|Department|  Age|            Height|Score1|\n+----------+-----+------------------+------+\n|        SC|24.75|1.6900000000000002|   8.0|\n|        AR| 23.0|              1.71|   8.0|\n+----------+-----+------------------+------+"
  },
  {
    "objectID": "english/spark_map.html#you-define-the-calculation-and-spark_map-distributes-it",
    "href": "english/spark_map.html#you-define-the-calculation-and-spark_map-distributes-it",
    "title": "spark_map()",
    "section": "You define the calculation and spark_map() distributes it",
    "text": "You define the calculation and spark_map() distributes it\nAll spark_map() does is apply any function to a set of columns in your DataFrame. And this function can be any function, as long as it is an aggregator function (that is, a function that can be used inside the pyspark.sql.dataframe.DataFrame.agg() and pyspark.sql.group.GroupedData.agg() methods). As long as your function meets this requirement, you can define whatever calculation formula you want, and use spark_map() to spread that calculation over multiple columns.\nAs an example, suppose you needed to use a little of inference to test whether the average of the various Student Scores significantly deviates from 6, through the statistic produced by a t test:\n\nfrom spark_map.mapping import starts_with\n\ndef t_test(x, value_test = 6):\n  return ( F.mean(x) - F.lit(value_test) ) / ( F.stddev(x) / F.sqrt(F.count(x)) )\n\nresults = spark_map(students, starts_with(\"Score\"), t_test)\nresults.show(truncate = False)\n\nSelected columns by `spark_map()`: Score1, Score2, Score3, Score4\n\n\n\n+-----------------+------------------+-----------------+----------------+\n|Score1           |Score2            |Score3           |Score4          |\n+-----------------+------------------+-----------------+----------------+\n|3.464101615137754|2.7116307227332026|4.338609156373122|4.47213595499958|\n+-----------------+------------------+-----------------+----------------+"
  },
  {
    "objectID": "english/articles/building-mapping.html",
    "href": "english/articles/building-mapping.html",
    "title": "Building the mapping",
    "section": "",
    "text": "You need to provide a mapping to spark_map(). This mapping defines which columns spark_map() should apply the function given in the function argument. You can build this mapping using one of the mapping functions, which are as follows:\nAs a first example, you can use the at_position() function whenever you want to select columns by position. So if you want to select the first, second, third and fourth column, you give the respective indices of those columns to the at_position() function.\nOn the other hand, you may need to use another method to map the columns you are interested in. For example, the students DataFrame has 4 Score columns (Score1, Score2, Score3 and Score4), and we have two obvious ways to map all these columns. One way is using the starts_with() function, and another is using the matches() function. Both options below bring the same results."
  },
  {
    "objectID": "english/articles/building-mapping.html#the-mapping-class",
    "href": "english/articles/building-mapping.html#the-mapping-class",
    "title": "Building the mapping",
    "section": "The Mapping class",
    "text": "The Mapping class\nBasically, the mapping is just a small description containing the algorithm that should be used to find the columns and the value that will be passed on to this algorithm. As an example, the result of the expression at_position(3, 4, 5) is a small dict, containing two elements (fun and val). The fun element defines the method/algorithm to be used to find the columns, and the val element stores the value that will be passed to this method/algorithm.\n\nfrom spark_map.mapping import at_position\nat_position(3, 4, 5)\n\n{'fun': 'at_position', 'val': [2, 3, 4]}\n\n\nAs another example, the result of the expression matches('^Score') is quite similar. However, unlike the previous example that uses a method called at_position, this time, the method/algorithm to be used is called matches, and '^Score' is the value that will be passed to this method.\n\nmatches('^Score')\n\n{'fun': 'matches', 'val': '^Score'}\n\n\nTherefore, when you use one of the standard mapping functions that comes with spark_map package, a dict will be generated containing the name of the method/algorithm to be used to calculate the mapping, and, the input value that will be passed to this method. Upon receiving this dict, spark_map() or spark_across() will automatically search for the method/algorithm to be used inside the class Mapping, and it will execute this method. This means that all standard mapping methods/algorithms from the spark_map package are stored inside this Mapping class."
  },
  {
    "objectID": "english/articles/building-mapping.html#creating-your-own-mapping-method",
    "href": "english/articles/building-mapping.html#creating-your-own-mapping-method",
    "title": "Building the mapping",
    "section": "Creating your own mapping method",
    "text": "Creating your own mapping method\nDespite being quite useful, you might want to implement your own mapping algorithm, and you can easily do that. Just give to spark_map() or spak_across a dict similar to the dict’s produced by one of the standard mapping functions. That is, a dict containing an item named fun and another item named val.\nThe val item keeps getting the value/object you want to pass to your mapping algorithm/method. However, this time, the fun item must contain a function, not a method/algorithm name inside a string. That is, whenever spark_map() or spark_across() receives a string in this fun item, these functions assume that you are trying to use one of the standard mapping methods, and therefore, they start a search for this method inside the Mapping class.\nOn the other hand, if these functions receive a function in the fun item, then spark_map() or spark_across() will directly execute that function that is in the fun item, inserting the value you provided in the val item in the first argument of this function.\nHowever, this process has some important conditions. Every mapping function must always take three mandatory arguments: 1) value: an arbitrary value for the algorithm1 (which corresponds to to the value given in the val item); 2) cols: the Spark DataFrame column names as a list of strings2; 3) schema: the schema (or schema) of the Spark DataFrame3. Your mapping function should always have these three arguments, even if it doesn’t use all of them.\nTherefore, the value argument of every mapping function will always receive as input the value you passed to the val item of the initial dict. Now, the other two arguments, cols and schema, they will be automatically filled by spark_map() or spark_across(). In other words, spark_map() or spark_across() will automatically collect the values of these arguments for you.\nAs an example, the alphabetic_order() function below maps the column that is in a specific position in the alphabetical order. This alphabetic_order() function only uses the index and cols arguments, even though it receives three arguments. In other words, the schema argument is just there to satisfy the conditions of spark_map() and spark_across().\n\nfrom pyspark.sql.types import StructType\ndef alphabetic_order(index, cols: list, schema: StructType):\n    cols.sort()\n    return [cols[index]]\n\nOne other requirement that we haven’t covered yet is the return value of your mapping function. Your mapping function must always return a list of strings, which contain the names of the columns that were mapped by the function. If your mapping function does not find any columns during your search, it should return an empty list (i.e. []).\nAs a first demonstration, let’s apply this alphabetic_order() function in the DataFrame students as an example. The column list of this DataFrame is shown below:\n\nprint(students.columns)\n\n['StudentID', 'Name', 'Age', 'Heigth', 'Score1', 'Score2', 'Score3', 'Score4', 'Course', 'Department']\n\n\nNow, using the alphabetic_order() inside spark_map():\n\nspark_map(students, {'fun': alphabetic_order, 'val': 1}, max)\n\nSelected columns by `spark_map()`: Course\n\n\n\nDataFrame[Course: string]"
  },
  {
    "objectID": "english/articles/building-mapping.html#if-your-mapping-doesnt-find-any-columns",
    "href": "english/articles/building-mapping.html#if-your-mapping-doesnt-find-any-columns",
    "title": "Building the mapping",
    "section": "If your mapping doesn’t find any columns",
    "text": "If your mapping doesn’t find any columns\nOn the other hand, spark_map() will also raise a KeyError, in case the function you are using in your mapping doesn’t find any columns in your DataFrame. However, in this case, spark_map() sends a clear message that no columns were found using the mapping you defined. As an example, we could reproduce this error, when trying to map in the DataFrame students, all the columns that start with the string 'april'. A KeyError is raised in this case because there is no column in the students table whose name starts with the word “april”.\n\nspark_map(students, starts_with('april'), sum)\n\nKeyError: '`spark_map()` did not found any column that matches your mapping!'"
  },
  {
    "objectID": "english/starts_with.html",
    "href": "english/starts_with.html",
    "title": "starts_with()",
    "section": "",
    "text": "Map all columns of your Spark DataFrame whose name starts with a specific text. This function is one of several existing mapping functions (read the article “Building the mapping”)."
  },
  {
    "objectID": "english/starts_with.html#arguments",
    "href": "english/starts_with.html#arguments",
    "title": "starts_with()",
    "section": "Arguments",
    "text": "Arguments\n\ntext: a string containing the text you want to search for;"
  },
  {
    "objectID": "english/starts_with.html#details-and-examples",
    "href": "english/starts_with.html#details-and-examples",
    "title": "starts_with()",
    "section": "Details and examples",
    "text": "Details and examples\nTherefore, starts_with() is used to define which columns spark_map() will apply the given function to. This function performs the inverse process of ends_with(), that is, it searches for all columns whose name starts with a specific text. Therefore, with the expression starts_with(\"Score\"), starts_with() will map all columns whose name starts with the text \"Score\".\nDuring the mapping process, an exact match between the searched strings is always used. As a result, an expression like starts_with(\"Sales\") is not able to map columns like \"sales_brazil\", \"sales_colombia\" and \"sales_eua\", however it is able to map columns like \"Sales_france \" and \"Sales_russia\". If you need to be more flexible in your mapping, you’ll likely want to use the matches() function instead of starts_with().\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import mean\nfrom spark_map.functions import spark_map\nfrom spark_map.mapping import starts_with\nspark = SparkSession.builder.getOrCreate()\n\ndata = [\n  (2022, 1, 12300, 41000, 36981),\n  (2022, 2, 19120, 21300, 31000),\n  (2022, 3, 18380, 11500, 31281)\n]\n\nsales = spark.createDataFrame(data, ['year', 'month', 'Sales_france', 'sales_brazil', 'Sales_russia'])\n\nspark_map(sales, starts_with('Sales'), mean).show()\n\nSelected columns by `spark_map()`: Sales_france, Sales_russia\n\n\n\n[Stage 0:>                                                        (0 + 12) / 12]\n\n\n+------------+------------------+\n|Sales_france|      Sales_russia|\n+------------+------------------+\n|     16600.0|33087.333333333336|\n+------------+------------------+"
  },
  {
    "objectID": "english/spark_across.html",
    "href": "english/spark_across.html",
    "title": "spark_across()",
    "section": "",
    "text": "With spark_across() you can easily apply a function over multiple columns of a Spark DataFrame (i.e. DataFrame). In short, spark_across() use the withColumn() DataFrame method to apply your function over the mapped columns. This function is heavily inspired in the dplyr::across() function, from the R package dplyr1."
  },
  {
    "objectID": "english/spark_across.html#arguments",
    "href": "english/spark_across.html#arguments",
    "title": "spark_across()",
    "section": "Arguments",
    "text": "Arguments\n\ntable: a Spark DataFrame (i.e. pyspark.sql.dataframe.DataFrame);\nmapping: the mapping that defines the columns where you want to apply function (read the article “Building the mapping”);\nfunction: the function you want to apply to each column defined in mapping;\n**kwargs: named arguments to be passed to the function defined in function;"
  },
  {
    "objectID": "english/spark_across.html#details-and-examples",
    "href": "english/spark_across.html#details-and-examples",
    "title": "spark_across()",
    "section": "Details and examples",
    "text": "Details and examples\nAs an example, consider the students DataFrame below:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nd = [\n  (12114, 'Anne', 21, 1.56, 8, 9, 10, 9, 'Economics', 'SC'),\n  (13007, 'Adrian', 23, 1.82, 6, 6, 8, 7, 'Economics', 'SC'),\n  (10045, 'George', 29, 1.77, 10, 9, 10, 7, 'Law', 'SC'),\n  (12459, 'Adeline', 26, 1.61, 8, 6, 7, 7, 'Law', 'SC'),\n  (10190, 'Mayla', 22, 1.67, 7, 7, 7, 9, 'Design', 'AR'),\n  (11552, 'Daniel', 24, 1.75, 9, 9, 10, 9, 'Design', 'AR')\n]\n\ncolumns = [\n  'StudentID', 'Name', 'Age', 'Height', 'Score1',\n  'Score2', 'Score3', 'Score4', 'Course', 'Department'\n]\n\nstudents = spark.createDataFrame(d, columns)\nstudents.show(truncate = False)\n\n[Stage 0:>                                                          (0 + 1) / 1]                                                                                \n\n\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|StudentID|Name   |Age|Height|Score1|Score2|Score3|Score4|Course   |Department|\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|12114    |Anne   |21 |1.56  |8     |9     |10    |9     |Economics|SC        |\n|13007    |Adrian |23 |1.82  |6     |6     |8     |7     |Economics|SC        |\n|10045    |George |29 |1.77  |10    |9     |10    |7     |Law      |SC        |\n|12459    |Adeline|26 |1.61  |8     |6     |7     |7     |Law      |SC        |\n|10190    |Mayla  |22 |1.67  |7     |7     |7     |9     |Design   |AR        |\n|11552    |Daniel |24 |1.75  |9     |9     |10    |9     |Design   |AR        |\n+---------+-------+---+------+------+------+------+------+---------+----------+\n\n\n\nSuppose you want to cast all columns of this DataFrame students that starts with the text \"Score\" to the DoubleType(). The spark_across() function allows you to perform this calculation in an extremely simple and clear way, as shown below:\n\nfrom spark_map.functions import spark_across\nfrom spark_map.mapping import starts_with\n\ndef add_number_to_column(col, num = 1):\n    return col + num\n\nspark_across(\n    students,\n    starts_with('Score'),\n    add_number_to_column,\n    num = 5\n  )\\\n  .show(truncate = False)\n\nSelected columns by `spark_map()`: Score1, Score2, Score3, Score4\n\n\n\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|StudentID|Name   |Age|Height|Score1|Score2|Score3|Score4|Course   |Department|\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|12114    |Anne   |21 |1.56  |13    |14    |15    |14    |Economics|SC        |\n|13007    |Adrian |23 |1.82  |11    |11    |13    |12    |Economics|SC        |\n|10045    |George |29 |1.77  |15    |14    |15    |12    |Law      |SC        |\n|12459    |Adeline|26 |1.61  |13    |11    |12    |12    |Law      |SC        |\n|10190    |Mayla  |22 |1.67  |12    |12    |12    |14    |Design   |AR        |\n|11552    |Daniel |24 |1.75  |14    |14    |15    |14    |Design   |AR        |\n+---------+-------+---+------+------+------+------+------+---------+----------+"
  },
  {
    "objectID": "english/ends_with.html",
    "href": "english/ends_with.html",
    "title": "ends_with()",
    "section": "",
    "text": "Map all columns of your Spark DataFrame whose name ends with a specific text. This function is one of several existing mapping functions (read the article “Building the mapping”)."
  },
  {
    "objectID": "english/ends_with.html#arguments",
    "href": "english/ends_with.html#arguments",
    "title": "ends_with()",
    "section": "Arguments",
    "text": "Arguments\n\ntext: a string containing the text you want to search for;"
  },
  {
    "objectID": "english/ends_with.html#details-and-examples",
    "href": "english/ends_with.html#details-and-examples",
    "title": "ends_with()",
    "section": "Details and examples",
    "text": "Details and examples\nTherefore, ends_with() is used to define which columns spark_map() will apply the given function to. This function performs the inverse process of starts_with(), i.e. it searches for all columns whose name ends with a specific text. So, with the expression ends_with(\"Score\"), ends_with() will map all columns whose name ends with the text \"Score\".\nDuring the mapping process, an exact match between the searched strings is always used. As a result, an expression like ends_with(\"Sales\") is not able to map columns like \"brazil_sales\", \"colombia_sales\" and \"eua_sales\", however it is able to map columns like \"france_Sales \" and \"russia_Sales\". If you need to be more flexible in your mapping, you’ll likely want to use the matches() function instead of ends_with().\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import mean\nfrom spark_map.functions import spark_map\nfrom spark_map.mapping import ends_with\nspark = SparkSession.builder.getOrCreate()\n\ndata = [\n  (2022, 1, 12300, 41000, 36981),\n  (2022, 2, 19120, 21300, 31000),\n  (2022, 3, 18380, 11500, 31281)\n]\n\nsales = spark.createDataFrame(data, ['year', 'month', 'france_Sales', 'brazil_sales', 'russia_Sales'])\n\nspark_map(sales, ends_with('Sales'), mean).show()\n\nSelected columns by `spark_map()`: france_Sales, russia_Sales\n\n\n\n[Stage 0:>                                                        (0 + 12) / 12]\n\n\n+------------+------------------+\n|france_Sales|      russia_Sales|\n+------------+------------------+\n|     16600.0|33087.333333333336|\n+------------+------------------+"
  },
  {
    "objectID": "english/are_of_type.html",
    "href": "english/are_of_type.html",
    "title": "are_of_type()",
    "section": "",
    "text": "Map all columns of your Spark DataFrame that fit a certain data type (string, double, integer, etc.). This function is one of several existing mapping functions (read the article “Building the mapping”)."
  },
  {
    "objectID": "english/are_of_type.html#arguments",
    "href": "english/are_of_type.html#arguments",
    "title": "are_of_type()",
    "section": "Arguments",
    "text": "Arguments\n\narg_type: a string containing the name of the data type you want to search for (see the available values ​​in the “Details and Examples” section below);"
  },
  {
    "objectID": "english/are_of_type.html#details-and-examples",
    "href": "english/are_of_type.html#details-and-examples",
    "title": "are_of_type()",
    "section": "Details and examples",
    "text": "Details and examples\nTherefore, are_of_type() is used to define which columns spark_map() will apply the given function to. To use this function, you must provide one of the following values:\n\n\"string\": for columns of type pyspark.sql.types.StringType();\n\"int\": for columns of type pyspark.sql.types.IntegerType();\n\"long\": for columns of type pyspark.sql.types.LongType();\n\"double\": for columns of type pyspark.sql.types.DoubleType();\n\"date\": for columns of type pyspark.sql.types.DateType();\n\"datetime\": for columns of type pyspark.sql.types.TimestampType();\n\nThis means that are_of_type() accepts only one of the above values. If you provide a string that is not included in the above list, a ValueError is automatically raised by the function, as shown below:\n\nfrom spark_map.mapping import are_of_type\nare_of_type(\"str\")\n\nValueError: You must choose one of the following values: 'string', 'int', 'long', 'double', 'date', 'datetime'\n\n\nIn essence, are_of_type() uses your Spark DataFrame schema to determine which columns belong to the data type you have determined. Notice in the example below, that the column named \"date\" is mapped by spark_map(), even though this column is clearly a date column. This happens, because Spark is interpreting this column by the type pyspark.sql.types.StringType(), not by pyspark.sql.types.DateType().\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max\nfrom spark_map.functions import spark_map\nfrom spark_map.mapping import are_of_type\nspark = SparkSession.builder.getOrCreate()\n\ndata = [\n  (\"2022-03-01\", \"Luke\", 36981),\n  (\"2022-02-15\", \"Anne\", 31000),\n  (\"2022-03-12\", \"Bishop\", 31281)\n]\n\nsales = spark.createDataFrame(data, ['date', 'name', 'value'])\n\nspark_map(sales, are_of_type(\"string\"), max).show()\n\nSelected columns by `spark_map()`: date, name\n\n\n\n[Stage 0:>                                                        (0 + 12) / 12]\n\n\n+----------+----+\n|      date|name|\n+----------+----+\n|2022-03-12|Luke|\n+----------+----+"
  },
  {
    "objectID": "english/at_position.html",
    "href": "english/at_position.html",
    "title": "at_position()",
    "section": "",
    "text": "Map the columns of your Spark DataFrame based on their numeric indexes (1st, 2nd, 3rd column, etc.). This function is one of several existing mapping functions (read the article “Building the mapping”)."
  },
  {
    "objectID": "english/at_position.html#arguments",
    "href": "english/at_position.html#arguments",
    "title": "at_position()",
    "section": "Arguments",
    "text": "Arguments\n\n*indexes: the column indexes (separated by commas);\nzero_index: boolean value (True or False) indicating whether the indexes provided in *indexes are zero-based or not (read the “Details” section below). By default, this argument is set to False;"
  },
  {
    "objectID": "english/at_position.html#details-and-examples",
    "href": "english/at_position.html#details-and-examples",
    "title": "at_position()",
    "section": "Details and examples",
    "text": "Details and examples\nTherefore, at_position() is used to define which columns spark_map() will apply the given function to. To use this function, you supply the numeric indexes, separated by commas, that represent the columns you want to map in spark_map().\nThe zero_index argument is optional, and determines whether the given column indexes will be based on a zero-start index system, or on a one-start index system. Python uses a zero-start index system, so the value 0 represents the first value of an object, while 1 represents the second value of an object, and so on.\nBut, the zero_index argument is set by default to False. Because of this, the at_position() function always initially works with an index system starting at one. So, in the expression at_position(3, 4, 5), the at_position() function will map the 3rd, 4th and 5th columns of your Spark DataFrame. However, if you want to override this behavior, and use Python’s default index system (starting at zero), just set this argument to True. In the example below, at_position() will map to the 2nd, 3rd and 4th column of the sales DataFrame.\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import mean\nfrom spark_map.functions import spark_map\nfrom spark_map.mapping import at_position\nspark = SparkSession.builder.getOrCreate()\n\ndata = [\n  (2022, 1, 12300, 41000, 36981),\n  (2022, 2, 19120, 21300, 31000),\n  (2022, 3, 18380, 11500, 31281)\n]\n\nsales = spark.createDataFrame(data, ['year', 'month', 'france_Sales', 'brazil_sales', 'russia_Sales'])\n\nspark_map(sales, at_position(1, 2, 3, zero_index = True), mean).show()\n\nSelected columns by `spark_map()`: month, france_Sales, brazil_sales\n\n\n\n[Stage 0:>                                                        (0 + 12) / 12]\n\n\n+-----+------------+------------+\n|month|france_Sales|brazil_sales|\n+-----+------------+------------+\n|  2.0|     16600.0|     24600.0|\n+-----+------------+------------+\n\n\n\n                                                                                \n\n\nWhen providing a zero index, you should always set the zero_index argument to True. Because when the zero_index argument is set to False, at_position() will automatically subtract 1 from all indexes, and, as a consequence, an index equal to zero becomes an index equal to -1, and negative indexes are not allowed by at_position(). See the example below:\n\nat_position(0, 2, 4)\n\nValueError: One (or more) of the provided indexes are negative! Did you provided a zero index, and not set the `zero_index` argument to True?\n\n\nFurthermore, any duplicated index is automatically eliminated by at_position(). See the example below in which the indexes 1 and 4 are repeated during the call to at_position(), but they are automatically eliminated in the function result.\n\nat_position(1, 1, 2, 3, 4, 4, 5)\n\n{'fun': 'at_position', 'val': [0, 1, 2, 3, 4]}\n\n\nFurthermore, the indexes given to at_position() must not be inside a list, if you make this mistake, the function will raise a ValueError, as shown below:\n\nat_position([4, 5, 6])\n\nValueError: Did you provided your column indexes inside a list to `at_position()`? You should not encapsulate these indexes inside a list. For example, if you want to select 1° and 3° columns, just do `at_position(1, 3)` instead of `at_position([1, 3])`.\n\n\nColumn indexes are a required argument. So, if you don’t provide any index, at_position() will necessarily raise a ValueError, as shown below:\n\nat_position(zero_index = True)\n\nValueError: You provided an empty list as input for `at_position()`. However, this list should not be empty!\n\n\nFurthermore, is important to be careful to select a set of indexes that are inside of your DataFrame boudaries. In other words, if you try to select the 10th column of a DataFrame that have only 5 columns, you will get an IndexError, telling that your index is out of range:\n\nspark_map(sales, at_position(10), mean)\n\nIndexError: list index out of range"
  }
]